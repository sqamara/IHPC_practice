{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_24_hour(time):\n",
    "    if(len(time) == 0):\n",
    "        return \"\"\n",
    "    pieces = time.split(':')\n",
    "    numeric_time = int(pieces[0]+pieces[1][:2])\n",
    "    if ('p' in time):\n",
    "        numeric_time += 1200\n",
    "        return numeric_time\n",
    "    elif ('a' in time):\n",
    "        return numeric_time\n",
    "    \n",
    "def clean_games(line):\n",
    "    data = line.split(',') # split\n",
    "    data = data[1:len(data)-1] # remove game # and empty col\n",
    "    \n",
    "    if (data[4] == 'W' and data[6] < data[7]): # move warriors score into single col\n",
    "        data[6], data[7] = data[6], data[7]\n",
    "        \n",
    "    data[0] = data[0].split() # expand game date\n",
    "    data[1] = data[1].split() # expand game time\n",
    "    if (len(data[1]) == 0): # no game time\n",
    "        data[1] =['','']\n",
    "\n",
    "    to_return = data[0] + data[1] + data[2:]\n",
    "    to_return[4] = to_24_hour(to_return[4])\n",
    "    del to_return[7] # delete box score\n",
    "    del to_return[6] # delete tv provider\n",
    "    del to_return[5] # delete time zone because all EST\n",
    "    \n",
    "    if(to_return[5] == '@'): \n",
    "        to_return[5] = 'away'\n",
    "    else: \n",
    "        to_return[5] ='home'\n",
    "        \n",
    "    if (to_return[2] != ''):\n",
    "        to_return[2] = int(to_return[2])\n",
    "    if (to_return[3] != ''):\n",
    "        to_return[3] = int(to_return[3])\n",
    "    if (to_return[4] != ''):\n",
    "        to_return[4] = int(to_return[4])\n",
    "    return to_return\n",
    "\n",
    "def remove_postgame_data(data):\n",
    "    return data[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_games_RDD = sc.textFile(\"./data/train\")\\\n",
    "    .filter(lambda line : line[0] != 'G')\\\n",
    "    .map(clean_games)\\\n",
    "    .map(remove_postgame_data)\\\n",
    "    .filter(lambda data : data[4] != '')\n",
    "\n",
    "playoffs_2016_RDD = sc.textFile(\"./data/test\")\\\n",
    "    .filter(lambda line : line[0] != 'G')\\\n",
    "    .map(clean_games)\\\n",
    "    .map(remove_postgame_data)\\\n",
    "    .filter(lambda data : data[4] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_games_RDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'day_of_week')\n",
      "(1, 'month')\n",
      "(2, 'day')\n",
      "(3, 'year')\n",
      "(4, 'time')\n",
      "(5, 'location')\n",
      "(6, 'opponent')\n",
      "(7, 'outcome')\n"
     ]
    }
   ],
   "source": [
    "# \"date time tv box score away opponent outcome OT score1 score2 season_wins season_losses streak\n",
    "data_strings = enumerate(\"day_of_week month day year time location opponent outcome OT warriors_score opponents_score season_wins season_losses streak\".split())\n",
    "for pair in data_strings:\n",
    "    if (pair[0] < 8):\n",
    "        print (pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wed', 'Oct', 28, 2009, 1930, 'home', 'Houston Rockets', 'L']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_games_RDD.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"day_of_week\", StringType(), True), \\\n",
    "    StructField(\"month\", StringType(), True), \\\n",
    "    StructField(\"day\", IntegerType(), True), \\\n",
    "    StructField(\"year\", IntegerType(), True), \\\n",
    "    StructField(\"time\", IntegerType(), True), \\\n",
    "    StructField(\"location\", StringType(), True), \\\n",
    "    StructField(\"opponent\", StringType(), True), \\\n",
    "    StructField(\"outcome\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = sqlContext.createDataFrame(all_games_RDD, schema)\n",
    "test_df = sqlContext.createDataFrame(playoffs_2016_RDD, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "day_of_week_indexer = StringIndexer(inputCol = 'day_of_week', outputCol = 'day_of_week_indexed')\n",
    "month_indexer = StringIndexer(inputCol = 'month', outputCol = 'month_indexed')\n",
    "location_indexer = StringIndexer(inputCol = 'location', outputCol = 'location_indexed')\n",
    "opponent_indexer = StringIndexer(inputCol = 'opponent', outputCol = 'opponent_indexed')\n",
    "outcome_indexer = StringIndexer(inputCol = 'outcome', outputCol = 'outcome_indexed')\n",
    "\n",
    "string_cols = ['day_of_week_indexed', 'month_indexed', 'location_indexed', 'opponent_indexed']\n",
    "numeric_cols = ['day', 'year', 'time']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols =  string_cols + numeric_cols,\n",
    "    outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(labelCol = 'outcome_indexed', featuresCol = 'features')\n",
    "\n",
    "pipeline = Pipeline(stages=[day_of_week_indexer, \n",
    "                            month_indexer, \n",
    "                            location_indexer,\n",
    "                            opponent_indexer,\n",
    "                            outcome_indexer,\n",
    "                            assembler, \n",
    "                            classifier])\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = (model.transform(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predictions.select(\"prediction\", \"outcome_indexed\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IndexToString is expirimental\n",
    "# converter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_val\", labels=outcome_indexer)\n",
    "# converted = converter.transform(predictions)\n",
    "# predictions.select(\"predicted_val\", \"outcome\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.375 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome_indexed\", predictionCol=\"prediction\", metricName=\"precision\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
