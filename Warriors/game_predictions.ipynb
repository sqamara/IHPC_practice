{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  0 game num\n",
    "#  1 date\n",
    "#  2 time\n",
    "#  3 tv coverage\n",
    "#  4 'Box Score'\n",
    "#  5 home/away\n",
    "#  6 opponent\n",
    "#  7 W/L\n",
    "#  8 OT\n",
    "#  9 score1\n",
    "# 10 score2\n",
    "# 11 cumulative wins (afer game)\n",
    "# 12 cumulative losses (after game)\n",
    "# 13 streak\n",
    "# 14 empty \n",
    "def to_24_hour(time):\n",
    "    if(len(time) == 0):\n",
    "        return \"\"\n",
    "    pieces = time.split(':')\n",
    "    numeric_time = int(pieces[0]+pieces[1][:2])\n",
    "    if ('p' in time):\n",
    "        numeric_time += 1200\n",
    "        return numeric_time\n",
    "    elif ('a' in time):\n",
    "        return numeric_time\n",
    "    \n",
    "def clean_games(line):\n",
    "    data = line.split(',')\n",
    "    \n",
    "    data[1] = data[1].split()\n",
    "    data[1][2] = int(data[1][2])\n",
    "    data[1][3] = int(data[1][3])\n",
    "\n",
    "    if (data[2] != ''):\n",
    "        data[2] = to_24_hour(data[2].split()[0])\n",
    "        \n",
    "    if(data[5] == '@'): \n",
    "        data[5] = 'away'\n",
    "    else: \n",
    "        data[5] ='home'\n",
    "    \n",
    "    data[11] = int(data[11])\n",
    "    data[12] = int(data[12])\n",
    "    if (data[7] == 'W'):\n",
    "        if (data[9] < data[10]):\n",
    "            data[9], data[10] = data[10], data[9]\n",
    "        data[11] = data[11]-1\n",
    "    elif (data[7] == 'L'):\n",
    "        if (data[9] > data[10]):\n",
    "            data[9], data[10] = data[10], data[9]\n",
    "        data[12] = data[12]-1\n",
    "    \n",
    "    toReturn = data[1] + [data[2]] + data[5:7] + data[11:13] + [data[7]]\n",
    "    return toReturn\n",
    "\n",
    "#  0 day_of_week\n",
    "#  1 month\n",
    "#  2 day_of_month\n",
    "#  3 year\n",
    "#  4 time\n",
    "#  5 home/away\n",
    "#  6 opponent\n",
    "#  7 cumulative wins (before game)\n",
    "#  8 cumulative losses (before game)\n",
    "#  9 W/L\n",
    "\n",
    "\n",
    "all_games_RDD = sc.textFile(\"./data/train\")\\\n",
    "    .filter(lambda line : line[0] != 'G')\\\n",
    "    .map(clean_games)\\\n",
    "    .filter(lambda data : data[4] != '')\n",
    "\n",
    "playoffs_2016_RDD = sc.textFile(\"./data/test\")\\\n",
    "    .filter(lambda line : line[0] != 'G')\\\n",
    "    .map(clean_games)\\\n",
    "    .filter(lambda data : data[4] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_games_RDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sat', 'Apr', 16, 2016, 1530, 'home', 'Houston Rockets', 73, 9, 'W']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playoffs_2016_RDD.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_strings = 'day_of_week month day_of_month year time location opponent season_wins season_losses outcome'\n",
    "def map_type(data):\n",
    "    to_return = []\n",
    "    for d in data:\n",
    "        if (type(d) == str):\n",
    "            to_return.append(StringType())\n",
    "        elif (type(d) == int):\n",
    "            to_return.append(IntegerType())\n",
    "    return to_return\n",
    "\n",
    "types = map_type(all_games_RDD.collect()[0])\n",
    "\n",
    "struct_fields = []\n",
    "for title, sql_type in zip(data_strings.split(), types):\n",
    "    struct_fields.append(StructField(title,sql_type,True))\n",
    "schema = StructType(struct_fields)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------------+----+----+--------+--------------------+-----------+-------------+-------+\n",
      "|day_of_week|month|day_of_month|year|time|location|            opponent|season_wins|season_losses|outcome|\n",
      "+-----------+-----+------------+----+----+--------+--------------------+-----------+-------------+-------+\n",
      "|        Wed|  Oct|          28|2009|1930|    home|     Houston Rockets|          0|            0|      L|\n",
      "|        Fri|  Oct|          30|2009|1900|    away|        Phoenix Suns|          0|            1|      L|\n",
      "|        Wed|  Nov|           4|2009|1930|    home|   Memphis Grizzlies|          0|            2|      W|\n",
      "|        Fri|  Nov|           6|2009|1930|    home|Los Angeles Clippers|          1|            2|      L|\n",
      "|        Sun|  Nov|           8|2009|1800|    away|    Sacramento Kings|          1|            3|      L|\n",
      "+-----------+-----+------------+----+----+--------+--------------------+-----------+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = sqlContext.createDataFrame(all_games_RDD, schema)\n",
    "test_df = sqlContext.createDataFrame(playoffs_2016_RDD, schema)\n",
    "test_df.cache()\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "day_of_week_indexer = StringIndexer(inputCol = 'day_of_week', outputCol = 'day_of_week_indexed')\n",
    "month_indexer = StringIndexer(inputCol = 'month', outputCol = 'month_indexed')\n",
    "location_indexer = StringIndexer(inputCol = 'location', outputCol = 'location_indexed')\n",
    "opponent_indexer = StringIndexer(inputCol = 'opponent', outputCol = 'opponent_indexed')\n",
    "outcome_indexer = StringIndexer(inputCol = 'outcome', outputCol = 'outcome_indexed')\n",
    "\n",
    "string_cols = ['day_of_week_indexed', 'month_indexed', 'location_indexed', 'opponent_indexed']\n",
    "numeric_cols = ['day_of_month', 'year', 'time', 'season_wins', 'season_losses']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols =  string_cols + numeric_cols,\n",
    "    outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(labelCol = 'outcome_indexed', featuresCol = 'features')\n",
    "\n",
    "pipeline = Pipeline(stages=[day_of_week_indexer, \n",
    "                            month_indexer, \n",
    "                            location_indexer,\n",
    "                            opponent_indexer,\n",
    "                            outcome_indexer,\n",
    "                            assembler, \n",
    "                            classifier])\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = (model.transform(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predictions.select(\"prediction\", \"outcome_indexed\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IndexToString is expirimental\n",
    "# converter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_val\", labels=outcome_indexer)\n",
    "# converted = converter.transform(predictions)\n",
    "# predictions.select(\"predicted_val\", \"outcome\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.375 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"outcome_indexed\", predictionCol=\"prediction\", metricName=\"precision\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/1.6.1/libexec/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree: Test Error = 0.375\n",
      "logistic_regression: Test Error = 0.375\n",
      "random_forest: Test Error = 0.375\n",
      "gradient_boosted_tree: Test Error = 0.458333\n",
      "multilayer_perceptron: Test Error = 0.375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import *\n",
    "decision_tree = DecisionTreeClassifier(labelCol = 'outcome_indexed')\n",
    "logistic_regression = LogisticRegression(labelCol=\"outcome_indexed\")\n",
    "random_forest = RandomForestClassifier(labelCol=\"outcome_indexed\")\n",
    "gradient_boosted_tree = GBTClassifier(labelCol=\"outcome_indexed\")\n",
    "layers = [9,6,5,4,3,2]\n",
    "multilayer_perceptron = MultilayerPerceptronClassifier(labelCol=\"outcome_indexed\", layers=layers)\n",
    "\n",
    "classifiers = [decision_tree, logistic_regression, random_forest, gradient_boosted_tree,\n",
    "               multilayer_perceptron]\n",
    "names = ['decision_tree', 'logistic_regression', 'random_forest','gradient_boosted_tree',\n",
    "               'multilayer_perceptron']\n",
    "output = \"\"\n",
    "for classifier, name in zip(classifiers, names):\n",
    "    pipeline = Pipeline(stages=[day_of_week_indexer, \n",
    "                            month_indexer, \n",
    "                            location_indexer,\n",
    "                            opponent_indexer,\n",
    "                            outcome_indexer,\n",
    "                            assembler, \n",
    "                            classifier])\n",
    "    model = pipeline.fit(train_df)\n",
    "    prediction = (model.transform(test_df))\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    output += name + \": Test Error = %g\\n\" % (1.0 - accuracy)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
